\documentclass[]{iat}

%Hier können weitere benötigte Pakete Eingebunden werden
%\usepackage[backend=biber,style=alphabetic,]{biblatex}
\usepackage{float}
\usepackage{listingsutf8}
\usepackage{color}
\usepackage{gensymb}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{subcaption}
%Namen eingeben / Insert Name
\renewcommand{\author}{Nico Spiske}
%Art der Arbeit / Scope (Project, Thesis...)
\providecommand{\scope}{Bachelorarbeit}
%Thema der Arbeit / Theme of Thesis
\renewcommand{\subject}{Lösungsansätze für die Regelung von dynamischen Systemen mittels
Reinforcement Learning Algorithmen}
%Schlagwörter / Keywords
\providecommand{\keywords}{}
%Literaturliste *.bib / Bibliography
\providecommand{\bibfile}{literature}
%Matrikelnummer / Student ID
\providecommand{\studentID}{4436923}
%Betreuer /& Tutors
\providecommand{\tutora}{M.Sc. Ricardo Bosold}
\providecommand{\tutorb}{M.Sc. Phillipp Hendrys}
%Prüfer / Examiner
\providecommand{\examinera}{Prof. Dr.-Ing. Kai Michels}
\providecommand{\examinerb}{Dr.-Ing. Dennis Pierl}
%Farben
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}
%lst überschrift
\captionsetup[lstlisting]{font={Large}}

\hypersetup{%
	pdftitle	={\subject -- \author -- \today},
	pdfauthor	={\author},
	pdfsubject	={\subject},
	pdfkeywords ={\keywords}
}

\addbibresource{literature.bib}

\setlength{\footheight}{21pt}

\begin{document}
%Pfad zu Grafiken:
\graphicspath{{./project_graphics/}}
% Sprachauswahl /Language Selection (ngerman/english)
\selectlanguage{ngerman}
% \pagenumbering{roman}
\input{iat_title}
%Urherberrechtserklärung / Confirmation of Conformity Comment if not needed
\input{declaration_conformity}

\tableofcontents

\newpage

\chapter{Einleitung}

\section{Einleitung und Motivation}

\section{Zielsetzung}
Das Ziel dieser Arbeit soll es sein, mittels eines Reinforcement Learning Agentens ein Beispielhaftes dynamisches Modell zu regeln. Gezeigt werden sollen außerdem unteranderem Methoden um dynamische Systeme mit Hilfe von bestehenden Methodiken zu erstellen und sie anschließend mit dem Agenten zu verbinden. Das dynamische Modell soll hierbei in eine Umgebung umtransformiert werden, das der Agent in ihr frei agieren und lernen kann, umso anschließend eine Regelung dieses Modells zu ermöglichen. Als Beispielhaftes dynamisches Modell soll hier ein Kräftemodell für ein Schiff verwendet werden, welches durch Störgrößen wie dem Wind beeinflusst wird. Der Agent soll die möglichkeithaben dieses Modell zu regeln indem er mit dem Schiffantrieb und seinem Ruder interagieren darf.
\section{Aufbau der Arbeit}
Der Aufbau dieser Arbeit soll vermitteln wie ein Reinforcement Learning Agent zusammen mit einer Umgebung implementiert werden kann. Geschehen soll jenes durch eine kurze Einführung in den allgemeinen Begriff des Machine Leranings. Gefolgt auf diese Einweisung soll der Leser einen Einblick in die Grundkonzepte und die dazu gehörige Terminologie des Reinforcement Learnings erhalten. Innerhalb des Einblicks zu den Grundkonzepten sollen Begriffe geklärt werden, damit anschließend auf die möglichen Arten des Reinforcement Learnings eingegangen werden kann. Innerhalb dieses Teils werden einige Algorithmen anhang von Pseudocode und deren Anwendungsfälle vorgestellt. Nach dieser Übersicht folgt ein Einblick in das Herzstück dieser Arbeit, die verwendete Implementierung eines Soft Actor-Critic (SAC) Agenten. Im Unterkapitel zu dem SAC wird vorgestellt, wie dieser funktioniert,

\chapter{Reinforcement Learning}
Im Zeitalter der Information, in welchem wir uns heutzutage befinden, gibt es eine unbegreiflich große und stätig wachsende Menge an Daten. Dies erkannte bereits der Trend- und Zukunftsforscher John Naisbitt im Jahre 1982 in seinem Buch Megatrends.
\begin{align*}
    \text{Wir ertrinken in Informationen, aber hungern nach Wissen. - John Naisbitt}
\end{align*}
Aufgrund dieser riesigen Menge an Daten, gibt es eine große Nachfrage nach automatisierten Methoden, um diese Daten letztendlich auch auswerten und analysieren zu können. Abhilfe für diese Nachfrage, beziehungsweise für dieses Problem soll das Teilgebiet des maschinellen Lernens aus dem Oberbegriff der künstlichen Intelligenz bieten. Das maschinelle Lernen bietet hierbei eine Facette aus verschiedenen Algorithmen und Methoden um innerhalb der untersuchten Daten das zu finden, was der Benutzer braucht. Hierbei können die Daten nach bereits bekannten Mustern untersucht werden, oder es kann nach unbekannten Mustern gesucht werden, es können auch vorhersagen getroffen werden aus dem bestehenden Datensatz umso zum Beispiel Marktforschung betreiben zu können. Wie sich das maschinelle Lernen als Oberthema nun zusammensetzt, und wo genau der Schwerpunkt dieser Arbeit, dass Reinforcement Learning sich als Begriff wieder findet, soll in diesem Kapitel verständigt werden.

\section{Maschinelles Lernen}
Das maschinelle Lernen auch Machine Learning oder kurz ML, kann im ganzen in drei Kategorien unterteilt werden. Hierbei gibt es das überwachte Lernen, unüberwachte Lernen und das bestärkte Lernen wobei letzeres das in dieser Arbeit fokusierte Reinforcement Learning entspricht, wie es im großteil der Literatur genannt wird. Bevor die tatsächlichen Grundlagen des Reinforcement Learnings erklärt werden sollen, soll außerdem noch einmal auf die beiden anderen Algorithmenarten eingegangen werden, um für eine Übersicht und somit für ein besseres Verständnis zu sorgen.\\
Machine Learning Konzepte welche sich innerhalb des Bereichs des Überwachten Lernens finden, sind mit an verbreitesten. Bei ihnen kann unterschieden werden zwischen Ansätzen, welche zur klassifizierung dienen, oder Ansätze welche Regressionsprobleme lösen sollen. Grundsätzlich kann gesagt werden das beide dieser Ansätze dennoch dem selben Prinzip folgen. Beim überwachten Lernen werden große Mengen an Daten verwendet um zum Beispiel neuronale Netzwerke zu trainieren. Die Datensätze entsprechen einer Abbildung, wobei hier die sogennanten Features auf die Labels abgebiltet werden. Wird nun ein neuronales Netzwerk mit diesem Ansatz trainiert, so werden dem neuronalen Netzwerk einfach gesagt am Eingang die Features gezeigt und am Ausgang die dazugehörigen Labels.
\begin{figure}[H]
    \includegraphics[scale=1]{graphics/neural_network.pdf}
    \centering
    \caption{Ein einfaches neuronales Netzwerk mit einem Hidden Layer}
    \label{abb:nn}
\end{figure}

Im Detail ist das ganze Training deutlich komplizierter und hierbei werden sogennante Optimizer wie Adam, Nadam oder SGD verwendet, welche das Ziel haben die Kanten anzupassen und gegebenfalls andere Parameter anzupassen. Die Kanten zwischen den einzelnen Neuronen besitzen das komplette antrainierte Wissen, sie können dabei positiv, negativ oder neutral gewichtet sein. Ist eine Kante positiv Gewichtet, so hat sie einen erregenden Einfluss auf ein anknüpfendes Neuron. Eine negative Kante hingegen besitzt ein hemmendes Verhalten zum angebundenen Neuron. In einem neutralen Zustand besitzt die Kante keinen Einfluss auf das anschließende Neuron. Die Optimizer unterscheiden sich zueinander unteranderem dadurch, was für eine Implementierung eines Gradientenverfahrens sie verwenden. Dennoch sind die Gradientenverfahren der Kern eines jeden Optimizers und wird verwendet um den Trainingsprozess eines neuronalen Netzwerkes durchzuführen. Es wurde bereits beschrieben, wie im Grunde beim Training dem Eingang die Features gezeigt werden und gleichzeitig das dazugehörige Label dem Ausgang gezeigt wird. Was jedoch intern passiert, ist dass eine sogenannte Cost-Function durch die Optimizer untersucht wird. Das Ziel der Optimizer ist es dabei das lokale oder globale Minimum dieser Funktion zufinden, welche als variabeln alle Gewichtungen der Kanten und sonstige Parameter hat. Die Cost-Function beschreibt, wie gut ein neuronales Netzwerk seine Vorhersage für einen beliebigen Datensatz durchgeführt hat und gibt einen numerischen Wert dafür zurück. Mithilfe dieser Bewertung entscheiden die Optimizer dann, in wiefern die einzelnen Gewichtungen angepasst werden müssen um näher zum Minimum der Cost-Function zu rücken.

\section{Grundkonzept und Terminologie zum Reinforcement Learning}
Im wesentlichen g ibt es zwei Elemente, welche zusammen die Grundbasis setzen für alle Arten und Implementierungen im Bereich des Reinfocement Leranings. Diese zwei Elemente sind der Agent und die Umgebung. Die Umgebung, oft auch Environment genannt, ist das worin der Agent lebt und womit er interagieren kann. Ein Environment besitzt immer einen State $s$, welcher die kompletten aktuellen Informationen zu einem Environment trägt. Der Agent besitzt die möglichkeit das Environment und seinen State $s$ zu betrachten. Die Betrachtung des States aus perspektive des Agenten bekennt sich als die Observation $o$, welche im vergleich zu $s$, nicht alle Informationen zum Environment tragen muss, sondern auch nur eine Schnittmenge an Informationen zu $s$ besitzen kann. Wenn $o$ mit $s$ komplett übereinstimmt, wird von einer vollkommenden Oberservation gesprochen, ist es jedoch eine kleine Schnittmenge, so wird von einer partiellen Observation gesprochen. Wenn das Environment zum Beispiel ein Pendel beschreibt, dann würde $s$ hier allen Informationen entsprechen, welche ein Pendel beschreiben. Dies wären zum Beispiel die Fadenlänge, die Fläche vom Pendelkopf und der aktuelle Winkel des Pendels zu seiner Ruheposition. Hingegen könnte $o$ eine partielle Observation sein, welche lediglich den momentanen Winkel des Pendels zu seiner Ruheposition wiedergibt.


Im wesentlichen gibt es zwei Elemente, welche zusammen die Grundbasis setzen für alle Arten und Implementierungen im Bereich des Reinforcement Learnings. Diese zwei Elemente sind der Agent und die Umgebung. Die Umgebung, oft auch Environment genannt, ist das worin der Agent lebt und womit er interagieren kann. Innerhalb des Environments trifft der Agent seine Aktionen, welche Actions genannt werden, basierend darauf, was der Agent bereits weiß und was ihm am Ende die größte Belohnung gibt. Die Belohnung, oder auch der Reward, ist ein Wert welcher definiert wird durch eine Reward-Function. Wie die Reward-Function nun den Wert wirklich bestimmt ist abhängig vom aktuellen State des Environments und kann somit einen positiven, negativen oder neutralen Wert annehmen. Ein positiver Reward würde demnach dafür sprechen, dass die letzten Entscheidungen des Agents gut waren und er das Environment in einen guten State gebracht hat. Hingegen würde ein negativer Reward dafür sprechen das der Agent, keine guten Entscheidungen getroffen hat. Wichtig ist der Reward für den Agenten, da er immer das Hauptziel hat, diesen zu maximieren. Der Agent und sein Environment stehen somit immer in einem Zyklus worin es darum geht, dass der Agent eine solche Action ausführt, das dass Environment in einen State gebracht wird, womit der Agent seinen maximalen Reward bekommt.
\begin{figure}[H]
    \includegraphics[width=\textwidth]{graphics/agent_env_loop.pdf}
    \centering
    \caption{Visualiserung des Agent-Environment Zyklus}
    \label{abb:agent_env_loop}
\end{figure}
Da nun das Grundprinzip beschrieben wurde, soll in dem folgenden Text der ganze Prozess detailierter beschrieben werden, damit dann anschließend die Implementierung des Soft Actor-Critic Agenten betrachtet werden kann.
\section{Artes des Reinforcement Learnings}
\section{Soft Actor-Critic (SAC) Agenten}

\chapter{Methode und Umsetzung}

\section{Verwendete Software und Bibliotheken}
Der großteil der ganzen Arbeit wurde mithilfe der Programmiersprache Python umgesetzt. Python bietet den Vorteil an, dass es vergleichsweise eine recht einfache Sprache ist und außerdem eine ziemlich große Auswahl an Bibliotheken bietet im Bereich Machine Learning. Die verwendete Machine Learning Bibliothek in dieser Arbeit ist PyTorch. PyTorch bietet eine Umgebung an, mit welcher die neuronalen Netzwerke für den Agenten einfach und mit viel Freiheit definiert werden können. Graphen und die Videodateien für die visualisierung und auswertung einer Episode wurden erstellt mit Hilfe von Matplotlib. Da der Agent in dieser Arbeit eine Umgebung braucht, in welcher das dynamische Modell simuliert werden kann, wurde außerdem die Bibliothek Gym von OpenAI verwendet. Gym bietet ein Framework an, welches es dem Benutzer mit vordefinierten Methoden eine Umgebung zu erstellen. Es werden Methoden vorgegeben, welche unteranderm einen Schritt innerhalb einer Episode definieren, oder eine Methode um die ganze Umgebung wieder auf ihren Ursprung zurückzusetzten.
\section{Implementierung des Agenten}
\section{Umsetzung des Modells in Simulink}
Das in dieser Arbeit verwendete dynamische Modell, soll wie bereits in der Zielsetzung erwähnt ein Kräftemodell sein, welches eine Schiffsfahrt modelliert. Die Grundidee des Modells ist es dabei, dass das Schiff im Ursprung eines koordinatensystems startet und dann das Ziel hat, innerhalb eines vordefinierten Bereichs nach rechts zu einer Ziellinie zu fahren. Hierbei gibt es das zweirängige Ziel für das Schiff, dass es bei seiner überfahrt so mittig im vordefinierten Bereich bleiben soll wie möglich. Die folgende Abbildung \ref*{abb:boat_frame_example_2} soll eine visualierung für das gewollte Modell bieten.
\begin{figure}[H]
    \includegraphics[width=\textwidth]{graphics/boat_frame_example_2.png}
    \centering
    \caption{Visualiserung des Schiff-Agentens während einer überfahrt mit Windeinfluss (Blauer Pfeil) und richtungs Vektor (Roter Pfeil).}
    \label{abb:boat_frame_example_2}
\end{figure}
In Abbildung \ref*{abb:boat_frame_example_2} ist zu erkennen, wie das Schiff eine grüne Ziellinie hat, welche es unbedingt erreichen soll damit eine Episode innerhalb des Reinforcement Learnings erreicht ist. Außerdem ist durch schwarze Linien ein äußerer Bereich von -6 bis 6 gekennzeichnet, welches das Schiff nicht überschreiten soll. Ein überschreiten führt beim Training des Agenten zu einer frühzeitigen beeindigung der Episode. Des weiteren soll eine Störgröße das Schiff beziehungsweise den Agenten bei seiner überfahrt beeinflussen, damit der Agent lernen soll dieses Störgröße automatisch auszugleichen. Die Störgröße in Form von einem Wind im Modell vorhanden sein, der Wind kann dabei aus jeder Richtung wähen und verändert sich mit der Zeit in der Richtung und in der Windstärke. Innerhalb der Abbildung \ref*{abb:boat_frame_example_2} ist der Wind erkennbar an einem kleinen blauen Pfeil am Schiff selbst.\\
Die Modellbildung selbst findet nun statt aus der Perspektive des Schiffs, die tatsächliche Kinematik, sprich die zum Beispiel die Richtung in welche sich das Schiff dann im Environment bewegt wird in einem späteren Schritt dann umgerechnet.
Modelliert wird das Schiff durch drei Bewegungsgleichungen, welche die Kräfte in longitudinaler, transversaler und das Giermoment des Schiffs beschrieben.
\begin{figure}[H]
    \includegraphics[width=0.7\textwidth]{graphics/ship_eoms.pdf}
    \centering
    \caption{Die drei Bewegungsgleichungen des Schiffs.}
    \label{abb:ship_eoms}
\end{figure}
Die erste Bewegungsgleichung für die Bewegung in longitudinaler Richtung, setzt sich zusammen aus den folgenden Kräften.
\begin{align}
    F_{Lon} = F_{T} - F_{R} + F_{C} \label{eq:longitudinal_force}
\end{align}
Der Antrieb des Schiffs wird modelliert durch eine Gleichung, welche abhängig von der Drehgeschwindigkeit der Schiffsschreube einen Schub generiert.
\begin{align}
    F_T = K_T(J) \cdot \rho \cdot d^4 \cdot n^2 \cdot (1-t) \label{eq:thrust_force}
\end{align}
wobei,
\begin{enumerate}
    \item[] $\rho = 1$, die Wasserdichte
    \item[] $d = 1$, der Schraubendurchmesser
    \item[] $n$, die Drehzahl des Schaube mit $0 \leq n \leq 30$
    \item[] $t = 0.3$, der Leistungsabzug für die Schraube
\end{enumerate}
das $K_T(J)$ entspricht einer Kennlinie welche individuel für einen gegebenen Schaube per Experimenteller Messung bestimmt wird. Da dies für diese Arbeit jedoch nicht möglich ist, wurde dieses Kurve stattdesen mit einer Sinus-Kurve approximiert. Das $J$ in Gleichung \ref*{eq:thrust_force} der Gleichung, entspricht dem Vorschubsquotient, dieser Wert beschreibt wieviel tatsächlicher Schub von der Schaube aus generiert werden kann abhängig von der momentanen longtiudinalen Geschwindigkeit des Schiffs zur Drehgeschwindigkeit der Schiffschraube.
\begin{align}
    J = \frac{v_l \cdot (1-w)}{n \cdot d} \label{eq:thrust_coeffcient}
\end{align}
wobei,
\begin{enumerate}
    \item[] $v_l$, die longitudinale Geschwindigkeit des Schiffs
    \item[] $w$ = 0.3, Nachlaufströmungs Koeffizient
\end{enumerate}
Der Widerstand, welcher ausgehend vom Wasser auf das Schiff longitudinal wirkt und somit das Schiff auch ausbremst ist definiert als reguläre Widerstandsgleichung abhängig der momentanen Geschwindigkeit des Schiffs.
\begin{align}
    F_R = \frac{1}{2} \cdot v_l^2 \cdot \rho \cdot c_{SF} \cdot A_{SF} \label{eq:front_resistance}
\end{align}
wobei,
\begin{enumerate}
    \item[] $c_{SF} = 0.31$, der Reibungskoeffient des Schiffs von Vorne
    \item[] $A_{SF} = 20$, die Fläche des Schiffs von vorne
\end{enumerate}
entspricht.


\section{Einbindung des Modells in Python}
Das funktionierende Simulink Modell muss für die verwendung zusammen mit dem Agenten, in Python umgesetzt werden. Damit dies gelingen kann, wurde festgelegt, dass das Ziel sein soll einige Blöcke aus Simulink nachzuahmen umso eine fast identische Funktionalität zuhaben wie sie in Simulink vorliegt. Da innerhalb von Simulink Blöcke verwendet werden, welche verbunden werden durch Kanten und dies in Python nicht möglich ist. Werden innerhalb von Python stattdesen Klassenattribute verwendet um zum Beispiel die gesamt Beschleunigung eines Modells zu definieren. Ein solches Klassenattribute kann damit nun zum Beispiel verwendet werden um einen Feedback-Loop zu modellieren, wie es in Simulink mithilfe von zurücklaufenden Kanten gemacht wird. Die Integratoren welche in Simulink mit das Kernstück und eine der wichtigesten Blockarten ist, wurde in Python nachgebaut innerhalb einer Integrator Klasse. Die Klasse wurde so implementiert, dass für jeden Integratorblock, welcher aus einem Simulink Modell übernommen werden soll, ein Integrator Object definiert wird. Dies könnte dann folgend aussehen in einem Quellcode Beispiel.
\begin{lstlisting}[language=Python]
a_integrator = Integrator()
v_integrator = Integrator(initial_value=h_0)
\end{lstlisting}
Hier in diesem Quellcode Ausschnitt ist zu sehen, dass ein Integrator für jeweils die Beschleunigung und Geschwindigkeit eines Modells definiert sind. Zudem kommt hinzu, dass für den Integrator von Geschwindigkeit auf Position ein Initialwert gesetzt wurde von $h_0$. Es wurde außerdem aus Simulink die Integrator Fuktion nachgebaut um ein oberes und unteres Limit für die Integration zu setzen, auch diese würde als Argument übergeben werden können bei einem jeweiligen Integrator.\\
Die Integratoren selbst funktionieren nun so, dass sie eine eigene Klassenmethode besitzen, welche als Argument ein beliebiges Signal erwartet. Dieses Signal würde etwa der gesamt Beschleunigung oder Geschwindigkeit eines Modells entsprechen, hier in diesem Fall würden zum Beispiel also die Klassenattribute sein, welche bereits erwähnt wurden. Das eingehende Signal wird dann integriert nach folgender Formel.
\begin{align*}
    s' = \int_{t}^{t+dt} s \, dx
\end{align*}
Die Signal $s$ wird gesehen als eine Konstante, welche integriert wird vom Zeitschritt $t$ bis $t + dt$, wobei das $dt$ hier vorschreibt wie groß der Integrationschritt letztendlich sein soll. Innerhalb dieser Arbeit wurde $dt = 0.1$ gewählt, da es die ausreichende größe bietet um das gewählte Modell zu simulieren ohne großem Genauigkeitsverlust.
\newpage
\subsection{Testen des Integrators anhand eines einfachen Beispiels}
Um die Integrator Klasse in Python zu testen bevor sie für das richtige Schiffmodell verwendet wird, wurde ein einfaches Modell erstellt, in welchem ein Fallschirmspringer in einer bestimmten Höhe abspringt und irgendwann den Fallschirm öffnet und verlangsamt auf dem Boden ankommt. In diesem Fallschirm Modell wurde der Fallschirm so modelliert, das er ab eine Höhe von 1500 aufgeht und damit seine Fläche deutlich größer wird. Diese größere Fläche spiegelt sich dann wieder in der reduzierten Fallgeschwindigkeit des Fallschirmspringers. Beschrieben werden kann das Fallschirmmodell durch die folgende Kraftgleichung.
\begin{align*}
    F = -F_G + F_W
\end{align*}

Wobei das $F_G$ der Kraft entspricht, welche sich aus der Fallbeschleunigung $g$ und der Masse des Fallschirmspringers zusammensetzt. $F_W$ entspricht dem Wind Widerstand, welcher abhängig von der Fläche des Fallschirmspringers beziehungsweise des Fallschirms sich ändert. Sie wird modelliert durch die folgende Gleichung für den Widerstand.

\begin{align*}
    F_W = \frac{1}{2} \cdot v^2 \cdot \rho \cdot c \cdot A
\end{align*}
wobei hier $A$ die Fläche des Fallschirmspringers selbst oder des Fallschirmspringers mit Fallschirm ist. Selbes gilt auch für den Reibungskoeffient $c$.

In Simulink ergibt sich das gesamte Modell zu der folgenden Struktur, in welcher die einzelnen Kraftkomponenten und besonderen Funktionen wie das abbrechen des Modells sobald die Höhe 0 erreicht ist, Farblich hinterlegt sind.

\begin{figure}[H]
    \includegraphics[width=\textwidth]{graphics/parachute_simulink.png}
    \centering
    \caption{Das Fallschirm Modell in Simulink}
    \label{abb:parachute_simulink}
\end{figure}

Das gleiche Modell nun modelliert in Python ergibt sich zu folgenden Quellcode.

\begin{lstlisting}[language=Python]
from control_theory.control_blocks import Integrator

if __name__ == '__main__':
    # Define constants
    h_0 = 3000
    h_1 = 1500
    A_s = 0.5
    A_FS = 25
    m = 85
    c_w = 1.3
    p = 1.2
    g = 9.81
    t, t_max, dt = 0, 500, 0.1

    # Define integrator objects
    a_integrator = Integrator()
    v_integrator = Integrator(initial_value=h_0)

    total_a = 0
    while t <= t_max:
        # Integrate acceleration and velocity
        total_a -= g
        v = a_integrator.integrate_signal(total_a)
        s = v_integrator.integrate_signal(v)

        # Stop if ground reached
        if s < 0:
            break

        # Wind resistance
        if s < h_1:
            F_w = v**2 * 0.5 * p * c_w * A_FS
        else:
            F_w = v**2 * 0.5 * p * c_w * A_s
        # Turn force into acceleration
        total_a = F_w / m

        t += dt

\end{lstlisting}

Zusehen ist, dass das Modell in Python angetrieben wird durch eine $while$-Schleife, welche terminiert sobald das $t_{max}$ erreicht ist. Innerhalb der $while$-Schleife wird dafür die Laufvariabel $t$ inkrementiert durch ein zuvor festgelegtes $dt$, welches gleichzeitig auch als die Integrationsschrittgröße dient bei den Integratoren. Konstanten wurden außerhalb der $while$-Schleife definiert zusammen mit einem $total\_a$, welches die gesamt Beschleunigung des Modells beschreibt. Die Integratoren werden nun innerhalb der $while$-Schleife verwendet um die gesamt Beschleunigung in eine Geschwindigkeit und anschließend in die Höhe des Fallschirmspringers zu integrieren. Basierend auf diesen Methodenrückgaben können dann veränderungen an den Signalen vorgenommen werden um zum Beispiel das $F_W$ abhängig von der Höhe zu bestimmen.\\
Ein direkter Vergleich der Ergebnisse unter verwendung der gleichen konstanten bei den beiden Modellvarianten in Python und Simulink zeigt dann auch gleich, dass die Integratoren und der Ansatz für die Modellierung in Python korrekt sind. Anzumerken ist jedoch, dass die Zeitaxen sich bei den beiden Plots ein wenig unterscheiden, dies ist jedoch darauf zurückzuführen, wie Simulink die Zeitschritte definiert und wie sie im Python Modell definiert sind. In diesem Fall ist es einfach nur eine unterschiedliche Skalierung.
\begin{figure}[H]
    \includegraphics[width=\textwidth]{graphics/python_parachute_s_plot.png}
    \centering
    \caption{Plot der Höhe des Fallschirmspringers gegen die Zeit im Python Modell}
    \label{fig:python_parachute_s_plot}
\end{figure}

\begin{figure}[H]
    \includegraphics[width=\textwidth]{graphics/simulink_parachute_s_plot.png}
    \centering
    \caption{Plot der Höhe des Fallschirmspringers gegen die Zeit im Simulink Modell}
    \label{fig:simulink_parachute_s_plot}
\end{figure}

\newpage
\subsection{Aufbau des Schiffmodells}

\chapter{Ergebnisse}

\chapter{Diskussion}

\chapter{Zusammenfassung und Ausblick}

\end{document}